{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P3WM-hVOPfo"
   },
   "source": [
    "# Reinforcement Learning in Finite MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9_DLZvWQzhb"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnzUJeyJOPfq"
   },
   "source": [
    "## MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcWJSw_uOPfr"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './mvarl_hands_on/utils')\n",
    "import numpy as np\n",
    "from scipy.special import softmax # for SARSA\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "from cliffwalk import CliffWalk\n",
    "#from test_env import ToyEnv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ym-B_4HaOPfu"
   },
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVR5qYoLOPfv"
   },
   "outputs": [],
   "source": [
    "env = CliffWalk(proba_succ=0.98)\n",
    "\n",
    "####################################################################################\n",
    "# You probably want to test smaller enviroments before\n",
    "#env = ToyEnv1(gamma=0.95)\n",
    "####################################################################################\n",
    "\n",
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Usefult methods\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", policy)\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(4):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward)\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")\n",
    "print(env.R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUlNvT3cOPfx"
   },
   "source": [
    "## Question 1: Value iteration\n",
    "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
    "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition (reuse function implemented in 1)\n",
    "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|Q_n - Q^\\star\\|_{\\infty} = \\max_{s,a} |Q_n(s,a) - Q^\\star(s,a)|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8TLRx6MOPfy"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Point 1\n",
    "# --------------\n",
    "def bellman_operator(Q0, Ns, Na, R, P, gamma):\n",
    "    Q1 = np.zeros((Ns,Na))\n",
    "    for s in range(Ns):\n",
    "        for a in range(Na):\n",
    "            Q1[s,a] = np.dot(P[s,a,:],R[s,a,:]) + gamma*np.sum(P[s,a,:]*np.max(Q0, 1))\n",
    "            \n",
    "    greedy_policy = np.argmax(Q1, axis=1)\n",
    "    \n",
    "    return Q1, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jj65cQk5OPf0"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Point 2\n",
    "# --------------\n",
    "def value_iteration(Q0, env, max_iter, epsilon=1e-5):\n",
    "    Q_history = []\n",
    "    Q_history.append(Q0)\n",
    "    \n",
    "    P = env.P\n",
    "    Ns = env.Ns\n",
    "    Na = env.Na\n",
    "    R = env.R\n",
    "    gamma = env.gamma\n",
    "\n",
    "    Q1 = np.zeros((Ns,Na))\n",
    "    Q1[:] = Q0[:]\n",
    "    Q2 = np.zeros((Ns, Na))\n",
    "    residuals = np.zeros((max_iter))\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        Q2 , greedy_policy = bellman_operator(Q1, Ns, Na, R, P, gamma)\n",
    "        residuals[i] = np.max(np.abs(Q2-Q1))\n",
    "        \n",
    "        if residuals[i]<epsilon:\n",
    "            residuals = residuals[:i+1]\n",
    "            break\n",
    "        \n",
    "        Q1[:] = Q2[:]\n",
    "        \n",
    "        Q_history.append(Q2)\n",
    "    Q = Q2\n",
    "    return Q, greedy_policy, Q_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_lBe6q6OPf2"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Point 3\n",
    "# --------------\n",
    "with open(\"./mvarl_hands_on/data/Q_opts.json\", \"r\") as fp:\n",
    "    Qopts = json.load(fp)\n",
    "Qstar = Qopts[\"{}_{}\".format(type(env).__name__,env.gamma)]\n",
    "\n",
    "Ns = env.Ns\n",
    "Na = env.Na\n",
    "\n",
    "Q0 = np.zeros((Ns,Na))\n",
    "max_iter = 1500\n",
    "Q, greedy_policy, Q_history = value_iteration(Q0,env,max_iter)\n",
    "\n",
    "norm_values = [np.max(np.abs(Q_history[i] - Q_history[-1])) for i in range(len(Q_history)-1)]\n",
    "\n",
    "plt.plot(norm_values)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title(\"Q-learning: Convergence of Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbzFs3tDvcJy"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "for i in range(50):\n",
    "    action = greedy_policy[state]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azfXmRzZOPf4"
   },
   "source": [
    "## Question 2: Q learning\n",
    "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
    "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
    "\n",
    "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
    "  - Plot the error in Q-functions over iterations\n",
    "  - Plot the sum of rewards as a function of iteration\n",
    "\n",
    "\n",
    "$\\epsilon$-greedy policy:\n",
    "$$\n",
    "\\pi(s) = \\begin{cases}\n",
    "\\max_a Q(s,a) & \\text{w.p.} \\epsilon\\\\\n",
    "\\text{random action} & \\text{w.p.} 1- \\epsilon\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_t3WqIt-OPf5"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Q-Learning\n",
    "# ---------------------------\n",
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    a = np.argmax(Q[s,:])\n",
    "    if(np.random.rand()<=epsilon): # random action\n",
    "        aa = np.random.randint(Na-1)\n",
    "        if aa==a:\n",
    "            a=Na-1\n",
    "        else: \n",
    "            a=aa\n",
    "    return a\n",
    "\n",
    "\n",
    "def Qlearning(Q0, max_steps, Q_opt):\n",
    "    Qql = Q0\n",
    "    norm_values = []\n",
    "    t = 1\n",
    "    alpha = 1\n",
    "    epsilon = 1\n",
    "    x = env.reset()\n",
    "    while t < max_steps:\n",
    "        if(t%10000==0):\n",
    "            epsilon = epsilon/2\n",
    "        a = epsilon_greedy(Qql,x,epsilon)\n",
    "        y,r,d,_ = env.step(a)\n",
    "        alpha = 1/(t**(1/24))\n",
    "        Qql[x][a] = Qql[x][a] + alpha * (r + env.gamma * np.max(Qql[y][:]) - Qql[x][a])\n",
    "        norm_values.append(np.abs(Qql - Q_opt).mean())\n",
    "        if d==True:\n",
    "            x = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "        t = t + 1\n",
    "        \n",
    "    return Qql, norm_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tCr5ZIjVS13"
   },
   "source": [
    "In the section above you have an implementation of Q-learning. The step size $\\alpha_t$ is set to $\\alpha_t = \\frac{1}{t^{1/24}}$. From a theoretical point of view, this is not a good choice, because this step size does not respect the robbins-monroe conditions. However, this heuristic seems to work well in practice. \\\n",
    "\n",
    "Moreover, the exploration-exploitation dilemma is done through the update of the variable epsilon. I have decided to devide epsilon by two after a certain number of time steps. A better way of doing Q-learning would be to consider epsilon as a function of a state and to adapt the update of epsilon given the number of times this state has been visited before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKTc5nWIOPf6"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Point 1\n",
    "# --------------\n",
    "# Number of Q learning steps\n",
    "max_steps = int(1e5)  \n",
    "# max_steps = 10\n",
    "\n",
    "Q0 = np.zeros((env.Ns, env.Na))\n",
    "# Use the previous code to verify the correctness of q learning\n",
    "Q_opt, pi_opt, Q_hist = value_iteration(Q0, env, 2000, epsilon=1e-8)\n",
    "\n",
    "Qql, norm_values = Qlearning(Q0, max_steps, Q_opt)\n",
    "#Qql2, norm_values = Qlearning(Qql, max_steps, Q_opt)\n",
    "#Qql3, norm_values = Qlearning(Qql2, max_steps, Q_opt)\n",
    "\n",
    "print(env.render())\n",
    "print(\"optimal policy: \", pi_opt)\n",
    "greedy_policy = np.argmax(Qql, axis=1)\n",
    "#greedy_policy = np.argmax(Qql2, axis=1)\n",
    "#greedy_policy = np.argmax(Qql3, axis=1)\n",
    "print(\"est policy:\", greedy_policy)\n",
    "\n",
    "plt.plot(norm_values)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title(\"Q-learning: Convergence of Q\")\n",
    "plt.show()\n",
    "\n",
    "# how confident are you in the performance of the algorithm? maybe a single run is not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4lM4tm4VQpk"
   },
   "source": [
    "To run the Q-learning algorithm several times, please uncomment the corresponding lines. I did it and it did improve the performances. But we are still very far from the results obtained in part 1. As I explained before, a better way to diminish the error would be to do a \"smarter\" exploration of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4R01F1YRVkCp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MVARL19_part1_LAHIRE.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
